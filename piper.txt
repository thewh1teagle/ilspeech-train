Cloud: cloud.vastai.com
Hardware: rtx 3080 ti 12GB vram
Image: vastai/pytorch:2.5.1-cuda-12.1.1
Time: fine tune took ~1.5 days until ~0.6 total loss

1. Prepare environment
    git clone https://github.com/thewh1teagle/piper -b hebrew
    cd piper
    sudo apt-get install espeak-ng -y
    pip install uv
    cd src/python
    uv venv
    uv pip install -e .
    ./build_monotonic_align.sh

2. Prepare dataset

When you download from google drive, add confirm=yes to the URL and use wget.
***Important
    The dataset **must** be normalized to 22.05 kHz and 16-bit signed integer format (int16) with full-range amplitude scaling. 
3. Prepare checkpoint
    wget https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/ryan/medium/epoch=4641-step=3104302.ckpt

4. Preprocess
    uv run python -m piper_train.preprocess \
    --language he \
    --input-dir ../../hebrew/dummy_dataset \
    --output-dir ./train \
    --dataset-format ljspeech \
    --single-speaker \
    --sample-rate 22050 \
    --phoneme-type raw \
    --max-workers 6

This will generate ./train folder along with config.json and files to train

5. Train
    uv run python -m piper_train \
        --dataset-dir "./train" \
        --accelerator 'gpu' \
        --devices 1 \
        --batch-size 24 \ # 16/12GB 24/16GB
        --validation-split 0 \
        --num-test-examples 0 \
        --max_epochs 990000 \
        --resume_from_checkpoint ./epoch=4641-step=3104302.ckpt \
        --checkpoint-epochs 1 \
        --precision 32

6. Check while train
    cat ../../etc/test_sentences/test_he.jsonl  | \
        python3 -m piper_train.infer \
            --sample-rate 22050 \
            --checkpoint ./train/lightning_logs/version_0/checkpoints/*.ckpt \
            --output-dir ./output \
            --length-scale 1.3

7. Check loss_disc_all graph and ensure it keep decreasing
    uv run tensorboard --logdir ./train/lightning_logs/
    
8. Export onnx
    uv run python -m piper_train.export_onnx ./train/lightning_logs/version_0/checkpoints/*.ckpt model.onnx
    cp ./train/config.json model.config.json

9. Use it with piper-onnx https://github.com/thewh1teagle/piper-onnx/tree/main/examples


Gotchas

If you can't see hebrew in the terminal

sudo locale-gen en_US.UTF-8
sudo update-locale LANG=en_US.UTF-8
export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8


Depcrecated
    uv pip install torchmetrics==0.11.4
    uv pip install "numpy<2"
    uv pip install pytorch-lightning
    cmake -B build .
    cmake --build build
    &./piper.exe --model en_US-bryce-medium.onnx --config config.json --text "Hello, world!" --output_file output.wav
    english, bryce medium


Send audio to telegram every one hour


Open https://t.me/BotFather and create bot
Send a message to the bot
Open https://api.telegram.org/bot<YOUR_BOT_TOKEN>/getUpdates
Copy you user ID



# Telegram Bot credentials
BOT_TOKEN="bot token"
CHAT_ID="chatid"

while true; do
  # Run inference
  cat ../../etc/test_sentences/test_he.jsonl | \
  python3 -m piper_train.infer \
      --sample-rate 22050 \
      --checkpoint ./train/lightning_logs/version_1/checkpoints/*.ckpt \
      --output-dir ./output \
      --length-scale 1.3

  # Send audio files
  for FILE in output/1.wav output/2.wav; do
    curl -s -X POST "https://api.telegram.org/bot${BOT_TOKEN}/sendAudio" \
      -F chat_id="${CHAT_ID}" \
      -F audio=@"${FILE}"
  done

  # Sleep for 20 minutes
  sleep 1200
done













Multi speaker:

export OMP_NUM_THREADS=1

    uv run python -m piper_train.preprocess \
    --language he \
    --input-dir ../../hebrew/dummy_dataset_multi \
    --output-dir ./train \
    --dataset-format ljspeech \
    --multi-speaker \
    --sample-rate 22050 \
    --speaker-id 1638 \
    --phoneme-type raw

# replace "speaker_id": 0 with "speaker_id": 7 in dataset.jsonl
# Note: the model designed in a way that the first number eg. 0 affect / has more knowledge and the latest numbers less
# Like if you want to train new voice without affect too much the whole model and voices, you can choose higher number
# But if you use too low number eg. 0 (the first) it will learn 'faster' but probably affect things


# Override config.json with original config of the model
wget https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/libritts_r/medium/config.json -O ./train/config.json
# Replace phoneme_type to raw
# fetch checkpoint
wget https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/libritts_r/medium/epoch=404-step=1887300.ckpt

uv run python -m piper_train \
    --dataset-dir "./train" \
    --accelerator 'gpu' \
    --devices 1 \
    --batch-size 24 \
    --validation-split 0 \
    --num-test-examples 0 \
    --max_epochs 990000 \
    --resume_from_single_speaker_checkpoint ./epoch=404-step=1887300.ckpt \
    --checkpoint-epochs 1 \
    --precision 32

Batch sizes: 16/12GB 24/16GB

# put "speaker_id": 7 in the test json file
cat ../../etc/test_sentences/test_he.jsonl  | \
        python3 -m piper_train.infer \
            --sample-rate 22050 \
            --checkpoint ./train/lightning_logs/version_0/checkpoints/*.ckpt \
            --output-dir ./output \
            --length-scale 1.3







# Train high quality model
# Note: I think that higher model requires more vram when training so you may need to lower batch size

wget https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_US/ljspeech/high/ljspeech-2000.ckpt
  uv run python -m piper_train \
        --dataset-dir "./train" \
        --accelerator 'gpu' \
        --devices 1 \
        --batch-size 24 \
        --validation-split 0 \
        --num-test-examples 0 \
        --max_epochs 990000 \
        --resume_from_checkpoint ./ljspeech-2000.ckpt \
        --quality high \
        --checkpoint-epochs 1 \
        --precision 32



# Upload to huggingface

    uv pip install huggingface_hub
    git config --global credential.helper store # Allow clone private repo from HF
    huggingface-cli login --token "token" --add-to-git-credential # https://huggingface.co/settings/tokens
    uv run huggingface-cli upload --repo-type model phonikud ./ckpt/path # upload contents of the folder

    # Fetch the model by
    git lfs install
    git clone https://huggingface.co/user/phonikud

    # Fetch file by
    huggingface-cli download --repo-type dataset user/some-dataset some_file.7z --local-dir .
    sudo apt install p7zip-full
    7z x some_file.7z


Error:
RuntimeError: Expected all tensors to be on the same device
Solution: export CUDA_VISIBLE_DEVICES="cuda:0"

Train on raw tokens:
(1) preprocess with --phoneme-type raw
(2) use hebrew/raw_tokens.py as uv run hebrew/raw_tokens.py metadata.csv new_metadata.csv # hebrew2piper.json will be created
(3) train normally
(4) use hebrew/hebrew2piper.py hebrew2piper.json "זֶה מוֹדֵל בְּעִבְרִית שֶׁמְּדַבֵּר"
(5) Paste it in hebrew/create_test.py and run it



add librosa
add onnxruntime
add piper_phonemize
